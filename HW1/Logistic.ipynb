{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train_df = pd.read_csv(train_url) # training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
    "test_df = pd.read_csv(test_url) # test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.00</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.00</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                      Name  \\\n",
       "886          887         0       2                     Montvila, Rev. Juozas   \n",
       "887          888         1       1              Graham, Miss. Margaret Edith   \n",
       "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890         1       1                     Behr, Mr. Karl Howell   \n",
       "890          891         0       3                       Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
       "886    male  27.0      0      0      211536  13.00   NaN        S  \n",
       "887  female  19.0      0      0      112053  30.00   B42        S  \n",
       "888  female   NaN      1      2  W./C. 6607  23.45   NaN        S  \n",
       "889    male  26.0      0      0      111369  30.00  C148        C  \n",
       "890    male  32.0      0      0      370376   7.75   NaN        Q  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info from .Describe\n",
    "For numerical value\n",
    "- Age features is has some NULL value\n",
    "- Classification Task (1: Survive, 0: Decease)\n",
    "- Fare should be correlated with Passenger Class\n",
    "\n",
    "For other categorical value\n",
    "- Most data of cabin also NULL value\n",
    "- Only 2 of embarked are NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n"
     ]
    }
   ],
   "source": [
    "# Simplest guess is to use Mean, Med, Mode\n",
    "# In this work \"Median\" is selected.\n",
    "age_med = train_df[\"Age\"].median()\n",
    "print(age_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Age\"] = train_df[\"Age\"].fillna(age_med)\n",
    "test_df[\"Age\"] = test_df[\"Age\"].fillna(age_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S    644\n",
      "C    168\n",
      "Q     77\n",
      "Name: Embarked, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"Embarked\"].value_counts())\n",
    "train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"S\")\n",
    "test_df[\"Embarked\"] = test_df[\"Embarked\"].fillna(\"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.15468749999992\n",
      "20.66218315217391\n",
      "13.675550101832997\n"
     ]
    }
   ],
   "source": [
    "# Fare / Pclass\n",
    "fareMean_P1 = train_df[train_df[\"Pclass\"] == 1][\"Fare\"].mean()\n",
    "fareMean_P2 = train_df[train_df[\"Pclass\"] == 2][\"Fare\"].mean()\n",
    "fareMean_P3 = train_df[train_df[\"Pclass\"] == 3][\"Fare\"].mean()\n",
    "print(fareMean_P1)\n",
    "print(fareMean_P2)\n",
    "print(fareMean_P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152   NaN\n",
      "Name: Fare, dtype: float64\n",
      "152    3\n",
      "Name: Pclass, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dealing with Fare Null \n",
    "print(test_df[test_df[\"PassengerId\"] == 1044]['Fare'])\n",
    "print(test_df[test_df[\"PassengerId\"] == 1044]['Pclass'])\n",
    "\n",
    "# Then\n",
    "test_df.loc[test_df[\"PassengerId\"] == 1044, \"Fare\"] = fareMean_P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152    13.67555\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df[\"PassengerId\"] == 1044]['Fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S    646\n",
      "C    168\n",
      "Q     77\n",
      "Name: Embarked, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"Embarked\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male      577\n",
      "female    314\n",
      "Name: Sex, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"Sex\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked\n",
    "for index, unique in enumerate(train_df[\"Embarked\"].unique()):\n",
    "    train_df.loc[train_df[\"Embarked\"]==unique, \"Embarked\"] = index\n",
    "    test_df.loc[test_df[\"Embarked\"]==unique, \"Embarked\"] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked\n",
    "for index, unique in enumerate(train_df[\"Sex\"].unique()):\n",
    "    train_df.loc[train_df[\"Sex\"]==unique, \"Sex\"] = index\n",
    "    test_df.loc[test_df[\"Sex\"]==unique, \"Sex\"] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris   0  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...   1  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina   1  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)   1  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry   0  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  \n",
       "0         A/5 21171   7.2500   NaN        0  \n",
       "1          PC 17599  71.2833   C85        1  \n",
       "2  STON/O2. 3101282   7.9250   NaN        0  \n",
       "3            113803  53.1000  C123        0  \n",
       "4            373450   8.0500   NaN        0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  0., 22.,  0.],\n",
       "       [ 1.,  1., 38.,  1.],\n",
       "       [ 3.,  1., 26.,  0.],\n",
       "       ...,\n",
       "       [ 3.,  1., 28.,  0.],\n",
       "       [ 1.,  0., 26.,  1.],\n",
       "       [ 3.,  0., 32.,  2.]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Age can be less than 1, thus dtype should be float.\n",
    "train_data = np.array(train_df[[\"Pclass\", \"Sex\", \"Age\", \"Embarked\"]].values, dtype=float)\n",
    "train_label = np.array(train_df[\"Survived\"].values, dtype=int)\n",
    "train_label = np.reshape(train_label, (len(train_label), 1))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3. ,  0. , 34.5,  2. ],\n",
       "       [ 3. ,  1. , 47. ,  0. ],\n",
       "       [ 2. ,  0. , 62. ,  2. ],\n",
       "       ...,\n",
       "       [ 3. ,  0. , 38.5,  0. ],\n",
       "       [ 3. ,  0. , 28. ,  0. ],\n",
       "       [ 3. ,  0. , 28. ,  1. ]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array(test_df[[\"Pclass\", \"Sex\", \"Age\", \"Embarked\"]].values, dtype=float)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Without validation split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2196bb73c10>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10,10,100)\n",
    "z = sigmoid(x)\n",
    "plt.plot(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute forward path function\n",
    "def random_init_param(X):\n",
    "    #size of X is [m, n] where m=sample, n=features\n",
    "    W = np.random.randn(len(X[0]), 1) # +1 for Bias term\n",
    "    return W\n",
    "\n",
    "def cost_function(X, Y, H, W):\n",
    "    # X : training data\n",
    "    # H : prediction (before sigmoid)\n",
    "    # Y : training label\n",
    "    # W : trainable parameters\n",
    "    m = len(Y)\n",
    "    devide_zeros_threshold = 1e-5 # Solve devide by zero problem\n",
    "    #L2 loss \n",
    "    L2loss = (np.dot((Y-sigmoid(H)).T, (Y-sigmoid(H))))/2 # /2 for derivative term\n",
    "    # or Logistic loss\n",
    "    Logistloss = np.dot(-Y.T, np.log(sigmoid(H)+devide_zeros_threshold))-np.dot((1-Y).T, np.log((1-sigmoid(H))+devide_zeros_threshold))\n",
    "    cost = (1/m)*Logistloss\n",
    "    grad = (1/m)*(np.dot(X.T, (sigmoid(H)-Y)))\n",
    "    return cost, grad\n",
    "    \n",
    "def main(X, Y, W):\n",
    "    h = np.dot(X, W)\n",
    "    cost, grad = cost_function(X, Y, h, W)\n",
    "    #update parameters\n",
    "    W = W - (LR)*grad\n",
    "    return W, cost\n",
    "\n",
    "#round function uses threshold = 0.5\n",
    "def predict(X, params):\n",
    "    h = np.dot(X, params)\n",
    "    return np.round(sigmoid(h))\n",
    "\n",
    "def accuracy(pred, y):\n",
    "    return np.squeeze(np.squeeze((sum(y == pred)/len(X))*100))\n",
    "\n",
    "def add_bias(X):\n",
    "    Bias = np.ones((len(X), 1))\n",
    "    res = np.concatenate((Bias, X), axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Logistic loss :  3.943719061644017\n",
      "Training accuracy = 62.40%\n",
      "\n",
      "Epoch: 1001\n",
      "Logistic loss :  0.7979615502668788\n",
      "Training accuracy = 59.71%\n",
      "\n",
      "Epoch: 2001\n",
      "Logistic loss :  0.7189747253646873\n",
      "Training accuracy = 60.94%\n",
      "\n",
      "Epoch: 3001\n",
      "Logistic loss :  0.6741785665526663\n",
      "Training accuracy = 62.18%\n",
      "\n",
      "Epoch: 4001\n",
      "Logistic loss :  0.6452009052264166\n",
      "Training accuracy = 64.31%\n",
      "\n",
      "Epoch: 5001\n",
      "Logistic loss :  0.6238022929527219\n",
      "Training accuracy = 64.31%\n",
      "\n",
      "Epoch: 6001\n",
      "Logistic loss :  0.6064648479873002\n",
      "Training accuracy = 64.31%\n",
      "\n",
      "Epoch: 7001\n",
      "Logistic loss :  0.5916591754656852\n",
      "Training accuracy = 64.53%\n",
      "\n",
      "Epoch: 8001\n",
      "Logistic loss :  0.5786778805038203\n",
      "Training accuracy = 64.42%\n",
      "\n",
      "Epoch: 9001\n",
      "Logistic loss :  0.5671552298210287\n",
      "Training accuracy = 69.25%\n",
      "\n",
      "Epoch: 10001\n",
      "Logistic loss :  0.5568704057695705\n",
      "Training accuracy = 70.15%\n",
      "\n",
      "Epoch: 11001\n",
      "Logistic loss :  0.5476670554220541\n",
      "Training accuracy = 70.15%\n",
      "\n",
      "Epoch: 12001\n",
      "Logistic loss :  0.5394205116009971\n",
      "Training accuracy = 73.85%\n",
      "\n",
      "Epoch: 13001\n",
      "Logistic loss :  0.5320244508763609\n",
      "Training accuracy = 73.85%\n",
      "\n",
      "Epoch: 14001\n",
      "Logistic loss :  0.525385361765111\n",
      "Training accuracy = 73.85%\n",
      "\n",
      "Epoch: 15001\n",
      "Logistic loss :  0.519420070154902\n",
      "Training accuracy = 75.53%\n",
      "\n",
      "Epoch: 16001\n",
      "Logistic loss :  0.5140544147276148\n",
      "Training accuracy = 80.47%\n",
      "\n",
      "Epoch: 17001\n",
      "Logistic loss :  0.509222333541866\n",
      "Training accuracy = 80.92%\n",
      "\n",
      "Epoch: 18001\n",
      "Logistic loss :  0.5048650935126953\n",
      "Training accuracy = 81.03%\n",
      "\n",
      "Epoch: 19001\n",
      "Logistic loss :  0.500930577375425\n",
      "Training accuracy = 81.03%\n",
      "\n",
      "Epoch: 20001\n",
      "Logistic loss :  0.49737260907533654\n",
      "Training accuracy = 81.03%\n",
      "\n",
      "Epoch: 21001\n",
      "Logistic loss :  0.4941503192423765\n",
      "Training accuracy = 81.03%\n",
      "\n",
      "Epoch: 22001\n",
      "Logistic loss :  0.49122755632756426\n",
      "Training accuracy = 81.03%\n",
      "\n",
      "Epoch: 23001\n",
      "Logistic loss :  0.48857234749957973\n",
      "Training accuracy = 81.03%\n",
      "\n",
      "Epoch: 24001\n",
      "Logistic loss :  0.4861564108039645\n",
      "Training accuracy = 81.03%\n",
      "\n",
      "Epoch: 25001\n",
      "Logistic loss :  0.48395471780694965\n",
      "Training accuracy = 81.03%\n",
      "\n",
      "Epoch: 26001\n",
      "Logistic loss :  0.48194510432764825\n",
      "Training accuracy = 81.14%\n",
      "\n",
      "Epoch: 27001\n",
      "Logistic loss :  0.4801079258727152\n",
      "Training accuracy = 81.14%\n",
      "\n",
      "Epoch: 28001\n",
      "Logistic loss :  0.4784257538880237\n",
      "Training accuracy = 80.47%\n",
      "\n",
      "Epoch: 29001\n",
      "Logistic loss :  0.47688310878843915\n",
      "Training accuracy = 80.36%\n",
      "\n",
      "Epoch: 30001\n",
      "Logistic loss :  0.47546622580069525\n",
      "Training accuracy = 80.47%\n",
      "\n",
      "Epoch: 31001\n",
      "Logistic loss :  0.4741628498669004\n",
      "Training accuracy = 80.25%\n",
      "\n",
      "Epoch: 32001\n",
      "Logistic loss :  0.47296205614469333\n",
      "Training accuracy = 80.13%\n",
      "\n",
      "Epoch: 33001\n",
      "Logistic loss :  0.47185409296220393\n",
      "Training accuracy = 80.58%\n",
      "\n",
      "Epoch: 34001\n",
      "Logistic loss :  0.470830244414281\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 35001\n",
      "Logistic loss :  0.46988271010401084\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 36001\n",
      "Logistic loss :  0.46900449983063724\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 37001\n",
      "Logistic loss :  0.4681893412966864\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 38001\n",
      "Logistic loss :  0.46743159915163784\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 39001\n",
      "Logistic loss :  0.4667262039070105\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 40001\n",
      "Logistic loss :  0.46606858944960206\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 41001\n",
      "Logistic loss :  0.46545463804776416\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 42001\n",
      "Logistic loss :  0.4648806318922625\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 43001\n",
      "Logistic loss :  0.4643432103407572\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 44001\n",
      "Logistic loss :  0.463839332145465\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 45001\n",
      "Logistic loss :  0.463366242039227\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 46001\n",
      "Logistic loss :  0.46292144113789957\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 47001\n",
      "Logistic loss :  0.46250266068841756\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 48001\n",
      "Logistic loss :  0.46210783875357553\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 49001\n",
      "Logistic loss :  0.4617350994778364\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 50001\n",
      "Logistic loss :  0.4613827346245046\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 51001\n",
      "Logistic loss :  0.4610491871143607\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 52001\n",
      "Logistic loss :  0.46073303633025237\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 53001\n",
      "Logistic loss :  0.46043298498188867\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 54001\n",
      "Logistic loss :  0.4601478473508708\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 55001\n",
      "Logistic loss :  0.45987653875833806\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 56001\n",
      "Logistic loss :  0.45961806611700734\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 57001\n",
      "Logistic loss :  0.4593715194462397\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 58001\n",
      "Logistic loss :  0.4591360642434309\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 59001\n",
      "Logistic loss :  0.4589109346177892\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 60001\n",
      "Logistic loss :  0.45869542710370254\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 61001\n",
      "Logistic loss :  0.4584888950806151\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 62001\n",
      "Logistic loss :  0.45829074373483386\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 63001\n",
      "Logistic loss :  0.4581004255061217\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 64001\n",
      "Logistic loss :  0.45791743596845513\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 65001\n",
      "Logistic loss :  0.457741310100038\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 66001\n",
      "Logistic loss :  0.4575716189026969\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 67001\n",
      "Logistic loss :  0.45740796633519376\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 68001\n",
      "Logistic loss :  0.45724998652889415\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 69001\n",
      "Logistic loss :  0.4570973412576547\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 70001\n",
      "Logistic loss :  0.45694971763683084\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 71001\n",
      "Logistic loss :  0.45680682602898137\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 72001\n",
      "Logistic loss :  0.45666839813622073\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 73001\n",
      "Logistic loss :  0.4565341852612687\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 74001\n",
      "Logistic loss :  0.45640395672111334\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 75001\n",
      "Logistic loss :  0.4562774983988596\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 76001\n",
      "Logistic loss :  0.45615461142080377\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 77001\n",
      "Logistic loss :  0.45603511094708843\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 78001\n",
      "Logistic loss :  0.4559188250654603\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 79001\n",
      "Logistic loss :  0.4558055937786912\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 80001\n",
      "Logistic loss :  0.4556952680771554\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 81001\n",
      "Logistic loss :  0.4555877090888889\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 82001\n",
      "Logistic loss :  0.45548278730019676\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 83001\n",
      "Logistic loss :  0.4553803818405442\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 84001\n",
      "Logistic loss :  0.4552803798260637\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 85001\n",
      "Logistic loss :  0.4551826757565448\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 86001\n",
      "Logistic loss :  0.4550871709612596\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 87001\n",
      "Logistic loss :  0.45499377308940353\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 88001\n",
      "Logistic loss :  0.45490239564132834\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 89001\n",
      "Logistic loss :  0.45481295753708695\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 90001\n",
      "Logistic loss :  0.45472538271913643\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 91001\n",
      "Logistic loss :  0.4546395997863201\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 92001\n",
      "Logistic loss :  0.454555541656517\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 93001\n",
      "Logistic loss :  0.45447314525557436\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 94001\n",
      "Logistic loss :  0.4543923512303529\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 95001\n",
      "Logistic loss :  0.4543131036839026\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 96001\n",
      "Logistic loss :  0.4542353499309638\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 97001\n",
      "Logistic loss :  0.45415904027214116\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 98001\n",
      "Logistic loss :  0.45408412778524215\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 99001\n",
      "Logistic loss :  0.45401056813240254\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 100001\n",
      "Logistic loss :  0.45393831938173523\n",
      "Training accuracy = 79.12%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########################   TRAIN HERE    #############################\n",
    "\n",
    "X = train_data\n",
    "Y = train_label\n",
    "# Add bias for training sample\n",
    "X = add_bias(X)\n",
    "W = random_init_param(X)\n",
    "W_lowest = np.zeros((len(X[0]), 1))\n",
    "\n",
    "LR = 0.001\n",
    "epoch = 100000\n",
    "best_loss = 1e6 \n",
    "\n",
    "\n",
    "for i in range(epoch+1):\n",
    "    W, cost = main(X, Y, W)\n",
    "    pred = predict(X, W)\n",
    "    if cost < best_loss:\n",
    "        W_lowest = W\n",
    "    if (i%1000 == 0): # Just for logging\n",
    "        print(\"Epoch:\", i+1) \n",
    "        acc = accuracy(pred, Y)\n",
    "        print(\"Logistic loss : \", np.squeeze(cost))\n",
    "        print(\"Training accuracy = {:.2f}%\\n\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on test set\n",
    "- 76.32% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = add_bias(test_data)\n",
    "test_pred = predict(X_test, W_lowest)\n",
    "test_pred = np.squeeze(np.array(test_pred, dtype=int))\n",
    "len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = list(zip(test_df['PassengerId'].values, test_pred))\n",
    "test_result = pd.DataFrame(result, columns=['PassengerId', 'Survived'], index=None)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv(\"test_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  3.,  0., 22.,  0.],\n",
       "       [ 1.,  1.,  1., 38.,  1.],\n",
       "       [ 1.,  3.,  1., 26.,  0.],\n",
       "       ...,\n",
       "       [ 1.,  3.,  1., 28.,  0.],\n",
       "       [ 1.,  1.,  0., 26.,  1.],\n",
       "       [ 1.,  3.,  0., 32.,  2.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature(X):\n",
    "    poly_X = np.zeros((len(X), len(X[0])+6))\n",
    "    for m in range(len(X)):\n",
    "        term_Age = np.power(X[m][2],2)\n",
    "        term_Age3 = np.power(X[m][2],3)\n",
    "        term_Age4 = np.power(X[m][2],4)\n",
    "        termAgeClass1 = X[m][2]*X[m][0]\n",
    "        termAgeClass2 = X[m][2]*X[m][1]\n",
    "        termAgeClass3 = X[m][2]*X[m][3]\n",
    "        poly_X[m][:4] = X[m]\n",
    "        poly_X[m][4:10] = [term_Age, term_Age3, term_Age4, termAgeClass1, termAgeClass2, termAgeClass3]\n",
    "    return poly_X\n",
    "\n",
    "# Sigmoid overflow from polynomial term, so \"Norm\" is must.\n",
    "def feature_norm(X):\n",
    "    X_norm = X\n",
    "    order=len(X[0])-4\n",
    "    mu = np.zeros((1, len(X_norm[0])))\n",
    "    sigma = np.zeros((1, len(X_norm[0])))\n",
    "    \n",
    "    mu = np.mean(X_norm, axis=0)\n",
    "    sigma = np.std(X_norm, axis=0)\n",
    "    \n",
    "    X_norm[:, 2] = (X[:, 2] - mu[2])/sigma[2]\n",
    "    for i in range(order):\n",
    "        X_norm[:, i+4] = (X[:, i+4] - mu[i+4])/sigma[i+4]\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Logistic loss :  1.0601119527880438\n",
      "Training accuracy = 52.97%\n",
      "\n",
      "Epoch: 1001\n",
      "Logistic loss :  0.5028315297133293\n",
      "Training accuracy = 76.32%\n",
      "\n",
      "Epoch: 2001\n",
      "Logistic loss :  0.4614120237499157\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 3001\n",
      "Logistic loss :  0.454156676414297\n",
      "Training accuracy = 78.00%\n",
      "\n",
      "Epoch: 4001\n",
      "Logistic loss :  0.4521206683546938\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 5001\n",
      "Logistic loss :  0.45118471185522957\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 6001\n",
      "Logistic loss :  0.4505849805634173\n",
      "Training accuracy = 78.90%\n",
      "\n",
      "Epoch: 7001\n",
      "Logistic loss :  0.4501396804649189\n",
      "Training accuracy = 78.90%\n",
      "\n",
      "Epoch: 8001\n",
      "Logistic loss :  0.449787097952114\n",
      "Training accuracy = 78.90%\n",
      "\n",
      "Epoch: 9001\n",
      "Logistic loss :  0.44949746499803245\n",
      "Training accuracy = 78.90%\n",
      "\n",
      "Epoch: 10001\n",
      "Logistic loss :  0.4492529271383494\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 11001\n",
      "Logistic loss :  0.44904160016559225\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 12001\n",
      "Logistic loss :  0.44885517577368983\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 13001\n",
      "Logistic loss :  0.4486876958238348\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 14001\n",
      "Logistic loss :  0.44853482508439757\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 15001\n",
      "Logistic loss :  0.44839338060313183\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 16001\n",
      "Logistic loss :  0.4482610106940638\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 17001\n",
      "Logistic loss :  0.4481359688208602\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 18001\n",
      "Logistic loss :  0.44801695124732477\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 19001\n",
      "Logistic loss :  0.44790297919687844\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 20001\n",
      "Logistic loss :  0.4477933128180274\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 21001\n",
      "Logistic loss :  0.4476873881893944\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 22001\n",
      "Logistic loss :  0.4475847711361916\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 23001\n",
      "Logistic loss :  0.4474851233568453\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 24001\n",
      "Logistic loss :  0.44738817757618277\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 25001\n",
      "Logistic loss :  0.4472937193190787\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 26001\n",
      "Logistic loss :  0.4472015735383474\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 27001\n",
      "Logistic loss :  0.44711159480000584\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 28001\n",
      "Logistic loss :  0.44702366007407585\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 29001\n",
      "Logistic loss :  0.44693766343286356\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 30001\n",
      "Logistic loss :  0.4468535121452003\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 31001\n",
      "Logistic loss :  0.4467711237921232\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 32001\n",
      "Logistic loss :  0.44669042412998416\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 33001\n",
      "Logistic loss :  0.44661134550062737\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 34001\n",
      "Logistic loss :  0.4465338256422068\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 35001\n",
      "Logistic loss :  0.44645780679366626\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 36001\n",
      "Logistic loss :  0.4463832350147427\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 37001\n",
      "Logistic loss :  0.44631005966442605\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 38001\n",
      "Logistic loss :  0.4462382329961935\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 39001\n",
      "Logistic loss :  0.4461677098395645\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 40001\n",
      "Logistic loss :  0.4460984473457244\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 41001\n",
      "Logistic loss :  0.44603040478093425\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 42001\n",
      "Logistic loss :  0.4459635433558146\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 43001\n",
      "Logistic loss :  0.44589782608176787\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 44001\n",
      "Logistic loss :  0.4458332176481232\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 45001\n",
      "Logistic loss :  0.44576968431528813\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 46001\n",
      "Logistic loss :  0.44570719382041873\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 47001\n",
      "Logistic loss :  0.4456457152930298\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 48001\n",
      "Logistic loss :  0.44558521917862215\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 49001\n",
      "Logistic loss :  0.4455256771688877\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 50001\n",
      "Logistic loss :  0.44546706213740755\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 51001\n",
      "Logistic loss :  0.44540934808001625\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 52001\n",
      "Logistic loss :  0.445352510059196\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 53001\n",
      "Logistic loss :  0.44529652415200643\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 54001\n",
      "Logistic loss :  0.4452413674011583\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 55001\n",
      "Logistic loss :  0.44518701776891834\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 56001\n",
      "Logistic loss :  0.4451334540935875\n",
      "Training accuracy = 78.79%\n",
      "\n",
      "Epoch: 57001\n",
      "Logistic loss :  0.44508065604834196\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 58001\n",
      "Logistic loss :  0.44502860410225614\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 59001\n",
      "Logistic loss :  0.4449772794833552\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 60001\n",
      "Logistic loss :  0.4449266641435606\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 61001\n",
      "Logistic loss :  0.444876740725412\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 62001\n",
      "Logistic loss :  0.4448274925304571\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 63001\n",
      "Logistic loss :  0.44477890348921406\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 64001\n",
      "Logistic loss :  0.44473095813261776\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 65001\n",
      "Logistic loss :  0.444683641564869\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 66001\n",
      "Logistic loss :  0.44463693943761073\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 67001\n",
      "Logistic loss :  0.4445908379253616\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 68001\n",
      "Logistic loss :  0.44454532370214117\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 69001\n",
      "Logistic loss :  0.4445003839192254\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 70001\n",
      "Logistic loss :  0.4444560061839739\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 71001\n",
      "Logistic loss :  0.44441217853967474\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 72001\n",
      "Logistic loss :  0.4443688894463557\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 73001\n",
      "Logistic loss :  0.44432612776251323\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 74001\n",
      "Logistic loss :  0.4442838827277119\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 75001\n",
      "Logistic loss :  0.444242143946013\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 76001\n",
      "Logistic loss :  0.44420090137018886\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 77001\n",
      "Logistic loss :  0.4441601452866851\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 78001\n",
      "Logistic loss :  0.44411986630129385\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 79001\n",
      "Logistic loss :  0.44408005532550104\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 80001\n",
      "Logistic loss :  0.4440407035634774\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 81001\n",
      "Logistic loss :  0.4440018024996791\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 82001\n",
      "Logistic loss :  0.44396334388702846\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 83001\n",
      "Logistic loss :  0.44392531973564753\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 84001\n",
      "Logistic loss :  0.44388772230211604\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 85001\n",
      "Logistic loss :  0.44385054407922875\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 86001\n",
      "Logistic loss :  0.44381377778622794\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 87001\n",
      "Logistic loss :  0.44377741635948775\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 88001\n",
      "Logistic loss :  0.44374145294362827\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 89001\n",
      "Logistic loss :  0.44370588088303836\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 90001\n",
      "Logistic loss :  0.44367069371378864\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 91001\n",
      "Logistic loss :  0.4436358851559136\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 92001\n",
      "Logistic loss :  0.4436014491060479\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 93001\n",
      "Logistic loss :  0.4435673796303959\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 94001\n",
      "Logistic loss :  0.4435336709580234\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 95001\n",
      "Logistic loss :  0.44350031747445096\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 96001\n",
      "Logistic loss :  0.4434673137155387\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 97001\n",
      "Logistic loss :  0.4434346543616455\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 98001\n",
      "Logistic loss :  0.44340233423205105\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 99001\n",
      "Logistic loss :  0.44337034827962857\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 100001\n",
      "Logistic loss :  0.44333869158575495\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 101001\n",
      "Logistic loss :  0.4433073593554477\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 102001\n",
      "Logistic loss :  0.44327634691271856\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 103001\n",
      "Logistic loss :  0.44324564969613234\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 104001\n",
      "Logistic loss :  0.4432152632545628\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 105001\n",
      "Logistic loss :  0.44318518324313405\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 106001\n",
      "Logistic loss :  0.44315540541934156\n",
      "Training accuracy = 78.56%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107001\n",
      "Logistic loss :  0.443125925639342\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 108001\n",
      "Logistic loss :  0.44309673985440495\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 109001\n",
      "Logistic loss :  0.4430678441075196\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 110001\n",
      "Logistic loss :  0.4430392345301475\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 111001\n",
      "Logistic loss :  0.44301090733911597\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 112001\n",
      "Logistic loss :  0.44298285883364547\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 113001\n",
      "Logistic loss :  0.44295508539250383\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 114001\n",
      "Logistic loss :  0.4429275834712828\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 115001\n",
      "Logistic loss :  0.44290034959978963\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 116001\n",
      "Logistic loss :  0.44287338037955026\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 117001\n",
      "Logistic loss :  0.44284667248141707\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 118001\n",
      "Logistic loss :  0.4428202226432782\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 119001\n",
      "Logistic loss :  0.4427940276678623\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 120001\n",
      "Logistic loss :  0.44276808442063526\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 121001\n",
      "Logistic loss :  0.4427423898277838\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 122001\n",
      "Logistic loss :  0.4427169408742841\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 123001\n",
      "Logistic loss :  0.4426917346020479\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 124001\n",
      "Logistic loss :  0.4426667681081467\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 125001\n",
      "Logistic loss :  0.4426420385431072\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 126001\n",
      "Logistic loss :  0.4426175431092771\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 127001\n",
      "Logistic loss :  0.4425932790592564\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 128001\n",
      "Logistic loss :  0.44256924369439216\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 129001\n",
      "Logistic loss :  0.4425454343633342\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 130001\n",
      "Logistic loss :  0.4425218484606475\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 131001\n",
      "Logistic loss :  0.4424984834254807\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 132001\n",
      "Logistic loss :  0.4424753367402862\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 133001\n",
      "Logistic loss :  0.44245240592959223\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 134001\n",
      "Logistic loss :  0.4424296885588213\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 135001\n",
      "Logistic loss :  0.4424071822331555\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 136001\n",
      "Logistic loss :  0.44238488459644565\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 137001\n",
      "Logistic loss :  0.44236279333016215\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 138001\n",
      "Logistic loss :  0.4423409061523869\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 139001\n",
      "Logistic loss :  0.4423192208168422\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 140001\n",
      "Logistic loss :  0.44229773511195725\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 141001\n",
      "Logistic loss :  0.44227644685997\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 142001\n",
      "Logistic loss :  0.44225535391606197\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 143001\n",
      "Logistic loss :  0.4422344541675253\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 144001\n",
      "Logistic loss :  0.44221374553296133\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 145001\n",
      "Logistic loss :  0.4421932259615069\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 146001\n",
      "Logistic loss :  0.4421728934320911\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 147001\n",
      "Logistic loss :  0.4421527459527167\n",
      "Training accuracy = 78.90%\n",
      "\n",
      "Epoch: 148001\n",
      "Logistic loss :  0.4421327815597692\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 149001\n",
      "Logistic loss :  0.4421129983173493\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 150001\n",
      "Logistic loss :  0.44209339431662975\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 151001\n",
      "Logistic loss :  0.44207396767523427\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 152001\n",
      "Logistic loss :  0.44205471653663886\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 153001\n",
      "Logistic loss :  0.4420356390695933\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 154001\n",
      "Logistic loss :  0.442016733467563\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 155001\n",
      "Logistic loss :  0.44199799794818895\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 156001\n",
      "Logistic loss :  0.4419794307527675\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 157001\n",
      "Logistic loss :  0.44196103014574634\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 158001\n",
      "Logistic loss :  0.4419427944142378\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 159001\n",
      "Logistic loss :  0.44192472186754805\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 160001\n",
      "Logistic loss :  0.44190681083672206\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 161001\n",
      "Logistic loss :  0.4418890596741031\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 162001\n",
      "Logistic loss :  0.441871466752906\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 163001\n",
      "Logistic loss :  0.4418540304668055\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 164001\n",
      "Logistic loss :  0.44183674922953536\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 165001\n",
      "Logistic loss :  0.4418196214745024\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 166001\n",
      "Logistic loss :  0.4418026456544109\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 167001\n",
      "Logistic loss :  0.4417858202408991\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 168001\n",
      "Logistic loss :  0.4417691437241871\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 169001\n",
      "Logistic loss :  0.4417526146127347\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 170001\n",
      "Logistic loss :  0.4417362314329102\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 171001\n",
      "Logistic loss :  0.44171999272866874\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 172001\n",
      "Logistic loss :  0.4417038970612396\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 173001\n",
      "Logistic loss :  0.44168794300882336\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 174001\n",
      "Logistic loss :  0.4416721291662971\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 175001\n",
      "Logistic loss :  0.4416564541449288\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 176001\n",
      "Logistic loss :  0.44164091657209886\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 177001\n",
      "Logistic loss :  0.44162551509102965\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 178001\n",
      "Logistic loss :  0.4416102483605232\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 179001\n",
      "Logistic loss :  0.44159511505470495\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 180001\n",
      "Logistic loss :  0.4415801138627754\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 181001\n",
      "Logistic loss :  0.4415652434887676\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 182001\n",
      "Logistic loss :  0.4415505026513117\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 183001\n",
      "Logistic loss :  0.4415358900834051\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 184001\n",
      "Logistic loss :  0.44152140453218897\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 185001\n",
      "Logistic loss :  0.44150704475873\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 186001\n",
      "Logistic loss :  0.4414928095378084\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 187001\n",
      "Logistic loss :  0.44147869765771025\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 188001\n",
      "Logistic loss :  0.44146470792002573\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 189001\n",
      "Logistic loss :  0.441450839139452\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 190001\n",
      "Logistic loss :  0.44143709014360066\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 191001\n",
      "Logistic loss :  0.44142345977281006\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 192001\n",
      "Logistic loss :  0.4414099468799621\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 193001\n",
      "Logistic loss :  0.44139655033030284\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 194001\n",
      "Logistic loss :  0.44138326900126773\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 195001\n",
      "Logistic loss :  0.4413701017823106\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 196001\n",
      "Logistic loss :  0.4413570475747364\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 197001\n",
      "Logistic loss :  0.441344105291538\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 198001\n",
      "Logistic loss :  0.44133127385723603\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 199001\n",
      "Logistic loss :  0.4413185522077227\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 200001\n",
      "Logistic loss :  0.4413059392901088\n",
      "Training accuracy = 79.35%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########################   TRAIN HERE    #############################\n",
    "\n",
    "X = train_data\n",
    "Y = train_label\n",
    "# Add bias for training sample\n",
    "X_poly = map_feature(X)\n",
    "X_poly_norm = feature_norm(X_poly)\n",
    "X_poly_norm = add_bias(X_poly_norm)\n",
    "# Params initialize\n",
    "W = random_init_param(X_poly_norm)\n",
    "W_lowest = np.zeros((len(X_poly_norm[0])+1, 1))\n",
    "\n",
    "LR = 0.005\n",
    "epoch = 200000\n",
    "best_loss = 1e6 \n",
    "\n",
    "for i in range(epoch+1):\n",
    "    W, cost = main(X_poly_norm, Y, W)\n",
    "    pred = predict(X_poly_norm, W)\n",
    "    if cost < best_loss:\n",
    "        W_lowest = W\n",
    "    if (i%1000 == 0): # Just for logging\n",
    "        print(\"Epoch:\", i+1)\n",
    "        acc = accuracy(pred, Y)\n",
    "        print(\"Logistic loss : \", np.squeeze(cost))\n",
    "        print(\"Training accuracy = {:.2f}%\\n\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_poly = map_feature(test_data)\n",
    "Xtest_poly_norm = feature_norm(Xtest_poly)\n",
    "Xtest_poly_norm = add_bias(Xtest_poly_norm)\n",
    "test_pred = predict(Xtest_poly_norm, W_lowest)\n",
    "test_pred = np.squeeze(np.array(test_pred, dtype=int))\n",
    "len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = list(zip(test_df['PassengerId'].values, test_pred))\n",
    "test_result = pd.DataFrame(result, columns=['PassengerId', 'Survived'], index=None)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv(\"test_prediction_poly.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age can be less than 1, thus dtype should be float.\n",
    "train_data = np.array(train_df[[\"Sex\", \"Age\"]].values, dtype=float)\n",
    "train_label = np.array(train_df[\"Survived\"].values, dtype=int)\n",
    "train_label = np.reshape(train_label, (len(train_label), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = np.array(test_df[[\"Sex\", \"Age\"]].values, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Logistic loss :  6.919759685453438\n",
      "Training accuracy = 38.38%\n",
      "\n",
      "Epoch: 1001\n",
      "Logistic loss :  0.6133621733651632\n",
      "Training accuracy = 66.33%\n",
      "\n",
      "Epoch: 2001\n",
      "Logistic loss :  0.6050096447915109\n",
      "Training accuracy = 67.56%\n",
      "\n",
      "Epoch: 3001\n",
      "Logistic loss :  0.5975192115992749\n",
      "Training accuracy = 70.82%\n",
      "\n",
      "Epoch: 4001\n",
      "Logistic loss :  0.5907887354940529\n",
      "Training accuracy = 71.83%\n",
      "\n",
      "Epoch: 5001\n",
      "Logistic loss :  0.5847289780633327\n",
      "Training accuracy = 72.73%\n",
      "\n",
      "Epoch: 6001\n",
      "Logistic loss :  0.5792620458062915\n",
      "Training accuracy = 74.07%\n",
      "\n",
      "Epoch: 7001\n",
      "Logistic loss :  0.5743199620865185\n",
      "Training accuracy = 74.75%\n",
      "\n",
      "Epoch: 8001\n",
      "Logistic loss :  0.5698433828426288\n",
      "Training accuracy = 75.31%\n",
      "\n",
      "Epoch: 9001\n",
      "Logistic loss :  0.565780458362673\n",
      "Training accuracy = 75.76%\n",
      "\n",
      "Epoch: 10001\n",
      "Logistic loss :  0.5620858348657238\n",
      "Training accuracy = 76.09%\n",
      "\n",
      "Epoch: 11001\n",
      "Logistic loss :  0.5587197850521008\n",
      "Training accuracy = 76.21%\n",
      "\n",
      "Epoch: 12001\n",
      "Logistic loss :  0.555647454759873\n",
      "Training accuracy = 76.21%\n",
      "\n",
      "Epoch: 13001\n",
      "Logistic loss :  0.5528382124172951\n",
      "Training accuracy = 76.21%\n",
      "\n",
      "Epoch: 14001\n",
      "Logistic loss :  0.5502650884510945\n",
      "Training accuracy = 76.43%\n",
      "\n",
      "Epoch: 15001\n",
      "Logistic loss :  0.5479042927713005\n",
      "Training accuracy = 76.66%\n",
      "\n",
      "Epoch: 16001\n",
      "Logistic loss :  0.5457347996348711\n",
      "Training accuracy = 76.99%\n",
      "\n",
      "Epoch: 17001\n",
      "Logistic loss :  0.5437379904287439\n",
      "Training accuracy = 77.10%\n",
      "\n",
      "Epoch: 18001\n",
      "Logistic loss :  0.5418973461134512\n",
      "Training accuracy = 77.33%\n",
      "\n",
      "Epoch: 19001\n",
      "Logistic loss :  0.5401981821806985\n",
      "Training accuracy = 77.44%\n",
      "\n",
      "Epoch: 20001\n",
      "Logistic loss :  0.5386274199794957\n",
      "Training accuracy = 77.78%\n",
      "\n",
      "Epoch: 21001\n",
      "Logistic loss :  0.5371733891493712\n",
      "Training accuracy = 77.89%\n",
      "\n",
      "Epoch: 22001\n",
      "Logistic loss :  0.5358256566691975\n",
      "Training accuracy = 78.00%\n",
      "\n",
      "Epoch: 23001\n",
      "Logistic loss :  0.534574878694593\n",
      "Training accuracy = 77.89%\n",
      "\n",
      "Epoch: 24001\n",
      "Logistic loss :  0.5334126719263702\n",
      "Training accuracy = 78.23%\n",
      "\n",
      "Epoch: 25001\n",
      "Logistic loss :  0.5323315017383488\n",
      "Training accuracy = 78.23%\n",
      "\n",
      "Epoch: 26001\n",
      "Logistic loss :  0.5313245847059963\n",
      "Training accuracy = 78.34%\n",
      "\n",
      "Epoch: 27001\n",
      "Logistic loss :  0.5303858035279144\n",
      "Training accuracy = 78.34%\n",
      "\n",
      "Epoch: 28001\n",
      "Logistic loss :  0.529509632629284\n",
      "Training accuracy = 78.45%\n",
      "\n",
      "Epoch: 29001\n",
      "Logistic loss :  0.5286910729879889\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 30001\n",
      "Logistic loss :  0.5279255949372176\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 31001\n",
      "Logistic loss :  0.5272090878788384\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 32001\n",
      "Logistic loss :  0.5265378159948448\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 33001\n",
      "Logistic loss :  0.5259083791739637\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 34001\n",
      "Logistic loss :  0.5253176784807609\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 35001\n",
      "Logistic loss :  0.524762885588326\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 36001\n",
      "Logistic loss :  0.5242414156754373\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 37001\n",
      "Logistic loss :  0.5237509033571909\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 38001\n",
      "Logistic loss :  0.5232891812762075\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 39001\n",
      "Logistic loss :  0.5228542610312783\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 40001\n",
      "Logistic loss :  0.522444316162926\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 41001\n",
      "Logistic loss :  0.5220576669519326\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 42001\n",
      "Logistic loss :  0.5216927668183411\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 43001\n",
      "Logistic loss :  0.521348190135518\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 44001\n",
      "Logistic loss :  0.5210226212972261\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 45001\n",
      "Logistic loss :  0.5207148448958542\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 46001\n",
      "Logistic loss :  0.5204237368874172\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 47001\n",
      "Logistic loss :  0.5201482566341001\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 48001\n",
      "Logistic loss :  0.519887439728275\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 49001\n",
      "Logistic loss :  0.5196403915133674\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 50001\n",
      "Logistic loss :  0.5194062812269246\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 51001\n",
      "Logistic loss :  0.5191843366999387\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 52001\n",
      "Logistic loss :  0.518973839554089\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 53001\n",
      "Logistic loss :  0.5187741208452246\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 54001\n",
      "Logistic loss :  0.5185845571072439\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 55001\n",
      "Logistic loss :  0.5184045667556643\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 56001\n",
      "Logistic loss :  0.5182336068146643\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 57001\n",
      "Logistic loss :  0.5180711699353666\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 58001\n",
      "Logistic loss :  0.5179167816766165\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 59001\n",
      "Logistic loss :  0.5177699980226036\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 60001\n",
      "Logistic loss :  0.517630403114407\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 61001\n",
      "Logistic loss :  0.5174976071749567\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 62001\n",
      "Logistic loss :  0.5173712446090492\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 63001\n",
      "Logistic loss :  0.5172509722619545\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 64001\n",
      "Logistic loss :  0.5171364678218437\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 65001\n",
      "Logistic loss :  0.5170274283527606\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 66001\n",
      "Logistic loss :  0.5169235689462073\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 67001\n",
      "Logistic loss :  0.5168246214805996\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 68001\n",
      "Logistic loss :  0.5167303334789145\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 69001\n",
      "Logistic loss :  0.5166404670558005\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 70001\n",
      "Logistic loss :  0.5165547979462722\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 71001\n",
      "Logistic loss :  0.5164731146088736\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 72001\n",
      "Logistic loss :  0.5163952173968693\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 73001\n",
      "Logistic loss :  0.5163209177916426\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 74001\n",
      "Logistic loss :  0.5162500376930192\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 75001\n",
      "Logistic loss :  0.5161824087617367\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 76001\n",
      "Logistic loss :  0.516117871809715\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 77001\n",
      "Logistic loss :  0.5160562762341874\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 78001\n",
      "Logistic loss :  0.5159974794921092\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 79001\n",
      "Logistic loss :  0.5159413466115844\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 80001\n",
      "Logistic loss :  0.5158877497373445\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 81001\n",
      "Logistic loss :  0.5158365677075735\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 82001\n",
      "Logistic loss :  0.5157876856596184\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 83001\n",
      "Logistic loss :  0.5157409946623333\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 84001\n",
      "Logistic loss :  0.5156963913730042\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 85001\n",
      "Logistic loss :  0.5156537777169762\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 86001\n",
      "Logistic loss :  0.515613060588269\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 87001\n",
      "Logistic loss :  0.5155741515696047\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 88001\n",
      "Logistic loss :  0.5155369666704126\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 89001\n",
      "Logistic loss :  0.5155014260814875\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 90001\n",
      "Logistic loss :  0.5154674539450923\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 91001\n",
      "Logistic loss :  0.5154349781393928\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 92001\n",
      "Logistic loss :  0.5154039300762019\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 93001\n",
      "Logistic loss :  0.5153742445110955\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 94001\n",
      "Logistic loss :  0.5153458593650331\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 95001\n",
      "Logistic loss :  0.5153187155566882\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 96001\n",
      "Logistic loss :  0.5152927568447537\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 97001\n",
      "Logistic loss :  0.5152679296795477\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 98001\n",
      "Logistic loss :  0.5152441830632888\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 99001\n",
      "Logistic loss :  0.5152214684184716\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 100001\n",
      "Logistic loss :  0.5151997394638019\n",
      "Training accuracy = 78.68%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########################   TRAIN HERE    #############################\n",
    "\n",
    "X = train_data\n",
    "Y = train_label\n",
    "# Add bias for training sample\n",
    "X = add_bias(X)\n",
    "W = random_init_param(X)\n",
    "W_lowest = np.zeros((len(X[0]), 1))\n",
    "\n",
    "LR = 0.001\n",
    "epoch = 100000\n",
    "best_loss = 1e6 \n",
    "\n",
    "\n",
    "for i in range(epoch+1):\n",
    "    W, cost = main(X, Y, W)\n",
    "    pred = predict(X, W)\n",
    "    if cost < best_loss:\n",
    "        W_lowest = W\n",
    "    if (i%1000 == 0): # Just for logging\n",
    "        print(\"Epoch:\", i+1) \n",
    "        acc = accuracy(pred, Y)\n",
    "        print(\"Logistic loss : \", np.squeeze(cost))\n",
    "        print(\"Training accuracy = {:.2f}%\\n\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute forward path function\n",
    "def random_init_param(X):\n",
    "    #size of X is [m, n] where m=sample, n=features\n",
    "    W = np.random.randn(len(X[0]), 1) # +1 for Bias term\n",
    "    return W\n",
    "\n",
    "def Linear_cost_function(X, Y, H, W):\n",
    "    # X : training data\n",
    "    # H : prediction (before sigmoid)\n",
    "    # Y : training label\n",
    "    # W : trainable parameters\n",
    "    m = len(Y)\n",
    "    devide_zeros_threshold = 1e-5 # Solve devide by zero problem\n",
    "    #L2 loss \n",
    "    L2loss = (np.dot((Y-H).T, (Y-H)))/2 # /2 for derivative term\n",
    "    cost = (1/m)*L2loss\n",
    "    grad = (1/m)*(np.dot(X.T, (H-Y)))\n",
    "    return cost, grad\n",
    "    \n",
    "def Linear_main(X, Y, W):\n",
    "    h = np.dot(X, W)\n",
    "    cost, grad = Linear_cost_function(X, Y, h, W)\n",
    "    #update parameters\n",
    "    W = W - (LR)*grad\n",
    "    return W, cost\n",
    "\n",
    "#round function uses threshold = 0.5\n",
    "def Linear_predict(X, params):\n",
    "    h = np.dot(X, params)\n",
    "    return np.round(h)\n",
    "\n",
    "def accuracy(pred, y):\n",
    "    return np.squeeze(np.squeeze((sum(Y == pred)/len(X))*100))\n",
    "\n",
    "def add_bias(X):\n",
    "    Bias = np.ones((len(X), 1))\n",
    "    res = np.concatenate((Bias, X), axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Logistic loss :  338.1890982575477\n",
      "Training accuracy = 11.67%\n",
      "\n",
      "Epoch: 1001\n",
      "Logistic loss :  0.24681468268424797\n",
      "Training accuracy = 58.70%\n",
      "\n",
      "Epoch: 2001\n",
      "Logistic loss :  0.1509986119616627\n",
      "Training accuracy = 61.50%\n",
      "\n",
      "Epoch: 3001\n",
      "Logistic loss :  0.12014824629763458\n",
      "Training accuracy = 64.42%\n",
      "\n",
      "Epoch: 4001\n",
      "Logistic loss :  0.10380171709933\n",
      "Training accuracy = 68.46%\n",
      "\n",
      "Epoch: 5001\n",
      "Logistic loss :  0.09460042850060653\n",
      "Training accuracy = 73.18%\n",
      "\n",
      "Epoch: 6001\n",
      "Logistic loss :  0.08915062581925871\n",
      "Training accuracy = 77.44%\n",
      "\n",
      "Epoch: 7001\n",
      "Logistic loss :  0.0857433535486274\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 8001\n",
      "Logistic loss :  0.08348959338089357\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 9001\n",
      "Logistic loss :  0.08191185031514847\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 10001\n",
      "Logistic loss :  0.08074548869028006\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 11001\n",
      "Logistic loss :  0.0798394158590518\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 12001\n",
      "Logistic loss :  0.07910493905742265\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 13001\n",
      "Logistic loss :  0.0784886150389182\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 14001\n",
      "Logistic loss :  0.07795740497056455\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 15001\n",
      "Logistic loss :  0.07749033473770861\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 16001\n",
      "Logistic loss :  0.077073697667354\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 17001\n",
      "Logistic loss :  0.07669823943711786\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 18001\n",
      "Logistic loss :  0.07635747881335332\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 19001\n",
      "Logistic loss :  0.07604669229971436\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 20001\n",
      "Logistic loss :  0.07576229312973483\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 21001\n",
      "Logistic loss :  0.07550144748021631\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 22001\n",
      "Logistic loss :  0.07526183481993834\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 23001\n",
      "Logistic loss :  0.07504149654019238\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 24001\n",
      "Logistic loss :  0.07483873902484216\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 25001\n",
      "Logistic loss :  0.07465207050121417\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 26001\n",
      "Logistic loss :  0.07448015898835357\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 27001\n",
      "Logistic loss :  0.07432180351982076\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 28001\n",
      "Logistic loss :  0.07417591379713115\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 29001\n",
      "Logistic loss :  0.07404149526324971\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 30001\n",
      "Logistic loss :  0.07391763771743029\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 31001\n",
      "Logistic loss :  0.0738035062933142\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 32001\n",
      "Logistic loss :  0.07369833405686756\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 33001\n",
      "Logistic loss :  0.0736014157510129\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 34001\n",
      "Logistic loss :  0.07351210238229665\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 35001\n",
      "Logistic loss :  0.07342979645029885\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 36001\n",
      "Logistic loss :  0.0733539476866661\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 37001\n",
      "Logistic loss :  0.07328404921245359\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 38001\n",
      "Logistic loss :  0.07321963404909132\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 39001\n",
      "Logistic loss :  0.07316027193544737\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 40001\n",
      "Logistic loss :  0.07310556641469079\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 41001\n",
      "Logistic loss :  0.07305515216216237\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 42001\n",
      "Logistic loss :  0.07300869253061053\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 43001\n",
      "Logistic loss :  0.07296587729280121\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 44001\n",
      "Logistic loss :  0.0729264205641926\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 45001\n",
      "Logistic loss :  0.07289005889041626\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 46001\n",
      "Logistic loss :  0.07285654948593244\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 47001\n",
      "Logistic loss :  0.0728256686115644\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 48001\n",
      "Logistic loss :  0.07279721007974613\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 49001\n",
      "Logistic loss :  0.07277098387729641\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 50001\n",
      "Logistic loss :  0.07274681489639649\n",
      "Training accuracy = 78.56%\n",
      "\n",
      "Epoch: 51001\n",
      "Logistic loss :  0.07272454176521774\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 52001\n",
      "Logistic loss :  0.072704015770343\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 53001\n",
      "Logistic loss :  0.07268509986375549\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 54001\n",
      "Logistic loss :  0.07266766774774669\n",
      "Training accuracy = 78.68%\n",
      "\n",
      "Epoch: 55001\n",
      "Logistic loss :  0.07265160303162096\n",
      "Training accuracy = 78.90%\n",
      "\n",
      "Epoch: 56001\n",
      "Logistic loss :  0.07263679845455943\n",
      "Training accuracy = 78.90%\n",
      "\n",
      "Epoch: 57001\n",
      "Logistic loss :  0.0726231551694495\n",
      "Training accuracy = 78.90%\n",
      "\n",
      "Epoch: 58001\n",
      "Logistic loss :  0.07261058208289577\n",
      "Training accuracy = 78.90%\n",
      "\n",
      "Epoch: 59001\n",
      "Logistic loss :  0.0725989952470035\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 60001\n",
      "Logistic loss :  0.07258831729887325\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 61001\n",
      "Logistic loss :  0.07257847694406287\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 62001\n",
      "Logistic loss :  0.0725694084805684\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 63001\n",
      "Logistic loss :  0.0725610513601447\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 64001\n",
      "Logistic loss :  0.07255334978403694\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 65001\n",
      "Logistic loss :  0.07254625233042362\n",
      "Training accuracy = 79.01%\n",
      "\n",
      "Epoch: 66001\n",
      "Logistic loss :  0.07253971161108384\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 67001\n",
      "Logistic loss :  0.07253368395499558\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 68001\n",
      "Logistic loss :  0.0725281291167536\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 69001\n",
      "Logistic loss :  0.07252301000785895\n",
      "Training accuracy = 79.12%\n",
      "\n",
      "Epoch: 70001\n",
      "Logistic loss :  0.07251829244908695\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 71001\n",
      "Logistic loss :  0.07251394494227953\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 72001\n",
      "Logistic loss :  0.07250993846003863\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 73001\n",
      "Logistic loss :  0.0725062462519161\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 74001\n",
      "Logistic loss :  0.07250284366580687\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 75001\n",
      "Logistic loss :  0.07249970798335165\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 76001\n",
      "Logistic loss :  0.07249681826825133\n",
      "Training accuracy = 79.35%\n",
      "\n",
      "Epoch: 77001\n",
      "Logistic loss :  0.07249415522647965\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 78001\n",
      "Logistic loss :  0.07249170107746093\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 79001\n",
      "Logistic loss :  0.07248943943535299\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 80001\n",
      "Logistic loss :  0.07248735519964224\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 81001\n",
      "Logistic loss :  0.07248543445432057\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 82001\n",
      "Logistic loss :  0.07248366437497131\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 83001\n",
      "Logistic loss :  0.072482033143143\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 84001\n",
      "Logistic loss :  0.0724805298674403\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 85001\n",
      "Logistic loss :  0.0724791445108042\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 86001\n",
      "Logistic loss :  0.07247786782349677\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 87001\n",
      "Logistic loss :  0.0724766912813426\n",
      "Training accuracy = 79.24%\n",
      "\n",
      "Epoch: 88001\n",
      "Logistic loss :  0.07247560702881459\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 89001\n",
      "Logistic loss :  0.07247460782658428\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 90001\n",
      "Logistic loss :  0.07247368700318639\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 91001\n",
      "Logistic loss :  0.07247283841047471\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 92001\n",
      "Logistic loss :  0.07247205638257233\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 93001\n",
      "Logistic loss :  0.07247133569804173\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 94001\n",
      "Logistic loss :  0.07247067154502229\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 95001\n",
      "Logistic loss :  0.07247005948910251\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 96001\n",
      "Logistic loss :  0.07246949544371245\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 97001\n",
      "Logistic loss :  0.0724689756428384\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 98001\n",
      "Logistic loss :  0.07246849661587813\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 99001\n",
      "Logistic loss :  0.07246805516446832\n",
      "Training accuracy = 79.46%\n",
      "\n",
      "Epoch: 100001\n",
      "Logistic loss :  0.07246764834112983\n",
      "Training accuracy = 79.46%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########################   TRAIN HERE    #############################\n",
    "\n",
    "X = train_data\n",
    "Y = train_label\n",
    "# Add bias for training sample\n",
    "X = add_bias(X)\n",
    "W = random_init_param(X)\n",
    "W_lowest = np.zeros((len(X[0]), 1))\n",
    "\n",
    "LR = 0.001\n",
    "epoch = 100000\n",
    "best_loss = 1e6 \n",
    "\n",
    "\n",
    "for i in range(epoch+1):\n",
    "    W, cost = Linear_main(X, Y, W)\n",
    "    pred = Linear_predict(X, W)\n",
    "    if cost < best_loss:\n",
    "        W_lowest = W\n",
    "    if (i%1000 == 0): # Just for logging\n",
    "        print(\"Epoch:\", i+1) \n",
    "        acc = accuracy(pred, Y)\n",
    "        print(\"Logistic loss : \", np.squeeze(cost))\n",
    "        print(\"Training accuracy = {:.2f}%\\n\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76183634],\n",
       "       [-0.18509332],\n",
       "       [ 0.49340374],\n",
       "       [-0.00486948],\n",
       "       [ 0.04927934]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight from Matrix derivation (Normal EQ.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_mat_deriv = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77654442],\n",
       "       [-0.18843944],\n",
       "       [ 0.49086711],\n",
       "       [-0.00505436],\n",
       "       [ 0.04911346]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_mat_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE of two weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.00023402)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = np.squeeze(np.dot((W_lowest-W_mat_deriv).T, W_lowest-W_mat_deriv))\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
